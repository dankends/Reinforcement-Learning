{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff3e675",
   "metadata": {},
   "source": [
    "# Complete Reinforcement Learning Project: Bandit Problem and Gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074cda44",
   "metadata": {},
   "source": [
    "\n",
    "This notebook demonstrates reinforcement learning techniques applied to two classic problems:\n",
    "1. **Bandit Problem**: Solving a 10-armed bandit problem with various exploration-exploitation strategies.\n",
    "2. **Gridworld**: Using dynamic programming to find optimal policies in a grid environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e33c58",
   "metadata": {},
   "source": [
    "## 1. Bandit Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4162a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "class Bandit:\n",
    "    def __init__(self, true_reward=0):\n",
    "        self.q_true = np.random.normal(true_reward, 1, 10)\n",
    "        self.q_estimate = np.zeros(10)\n",
    "        self.action_count = np.zeros(10)\n",
    "\n",
    "    def pull(self, action):\n",
    "        return np.random.normal(self.q_true[action], 1)\n",
    "\n",
    "    def update_estimates(self, action, reward):\n",
    "        self.action_count[action] += 1\n",
    "        self.q_estimate[action] += (1 / self.action_count[action]) * (reward - self.q_estimate[action])\n",
    "\n",
    "def epsilon_greedy(bandit, epsilon, n_pulls=1000):\n",
    "    rewards = []\n",
    "    for _ in range(n_pulls):\n",
    "        action = np.random.randint(10) if random.random() < epsilon else np.argmax(bandit.q_estimate)\n",
    "        reward = bandit.pull(action)\n",
    "        bandit.update_estimates(action, reward)\n",
    "        rewards.append(reward)\n",
    "    return np.cumsum(rewards) / (np.arange(n_pulls) + 1)\n",
    "\n",
    "def optimistic_initial_values(bandit, initial_value, n_pulls=1000):\n",
    "    bandit.q_estimate = np.ones(10) * initial_value\n",
    "    rewards = []\n",
    "    for _ in range(n_pulls):\n",
    "        action = np.argmax(bandit.q_estimate)\n",
    "        reward = bandit.pull(action)\n",
    "        bandit.update_estimates(action, reward)\n",
    "        rewards.append(reward)\n",
    "    return np.cumsum(rewards) / (np.arange(n_pulls) + 1)\n",
    "\n",
    "def ucb(bandit, c, n_pulls=1000):\n",
    "    rewards = []\n",
    "    for t in range(1, n_pulls + 1):\n",
    "        ucb_estimates = bandit.q_estimate + c * np.sqrt(np.log(t) / (bandit.action_count + 1e-5))\n",
    "        action = np.argmax(ucb_estimates)\n",
    "        reward = bandit.pull(action)\n",
    "        bandit.update_estimates(action, reward)\n",
    "        rewards.append(reward)\n",
    "    return np.cumsum(rewards) / (np.arange(n_pulls) + 1)\n",
    "\n",
    "def gradient_bandit(bandit, alpha, baseline=True, n_pulls=1000):\n",
    "    preferences = np.zeros(10)\n",
    "    rewards = []\n",
    "    average_reward = 0\n",
    "    for t in range(n_pulls):\n",
    "        exp_preferences = np.exp(preferences)\n",
    "        action_probs = exp_preferences / np.sum(exp_preferences)\n",
    "        action = np.random.choice(10, p=action_probs)\n",
    "        reward = bandit.pull(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if baseline:\n",
    "            average_reward += (reward - average_reward) / (t + 1)\n",
    "        \n",
    "        for a in range(10):\n",
    "            if a == action:\n",
    "                preferences[a] += alpha * (reward - (average_reward if baseline else 0)) * (1 - action_probs[a])\n",
    "            else:\n",
    "                preferences[a] -= alpha * (reward - (average_reward if baseline else 0)) * action_probs[a]\n",
    "    return np.cumsum(rewards) / (np.arange(n_pulls) + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7478ec21",
   "metadata": {},
   "source": [
    "### Bandit Problem Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416fe94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ε-Greedy Results\n",
    "epsilons = [0, 0.1, 0.01]\n",
    "rewards_epsilon_greedy = [epsilon_greedy(Bandit(), epsilon) for epsilon in epsilons]\n",
    "for i, reward in enumerate(rewards_epsilon_greedy):\n",
    "    plt.plot(reward, label=f'ε = {epsilons[i]}')\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.legend()\n",
    "plt.title(\"ε-Greedy Results\")\n",
    "plt.show()\n",
    "\n",
    "# Optimistic Initial Values\n",
    "rewards_optimistic = optimistic_initial_values(Bandit(), initial_value=5)\n",
    "plt.plot(rewards_optimistic, label=\"Optimistic Initial Value = 5\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.legend()\n",
    "plt.title(\"Optimistic Initial Values Results\")\n",
    "plt.show()\n",
    "\n",
    "# UCB Results\n",
    "c_values = [0.1, 1, 2]\n",
    "rewards_ucb = [ucb(Bandit(), c) for c in c_values]\n",
    "for i, reward in enumerate(rewards_ucb):\n",
    "    plt.plot(reward, label=f'c = {c_values[i]}')\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.legend()\n",
    "plt.title(\"UCB Results\")\n",
    "plt.show()\n",
    "\n",
    "# Gradient Bandit Results\n",
    "alphas = [0.1, 0.4]\n",
    "rewards_gradient_bandit = [gradient_bandit(Bandit(), alpha) for alpha in alphas]\n",
    "for i, reward in enumerate(rewards_gradient_bandit):\n",
    "    plt.plot(reward, label=f'alpha = {alphas[i]}')\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.legend()\n",
    "plt.title(\"Gradient Bandit Results\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ea8dd",
   "metadata": {},
   "source": [
    "## 2. Gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7459031",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Gridworld:\n",
    "    def __init__(self, size=5, gamma=1.0, theta=0.01):\n",
    "        self.size = size\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.state_values = np.zeros((size, size))\n",
    "        self.policy = np.full((size, size), 'up')\n",
    "\n",
    "    def policy_evaluation(self):\n",
    "        delta = self.theta\n",
    "        while delta >= self.theta:\n",
    "            delta = 0\n",
    "            for i in range(self.size):\n",
    "                for j in range(self.size):\n",
    "                    v = self.state_values[i, j]\n",
    "                    new_value = self.calculate_value(i, j)\n",
    "                    delta = max(delta, abs(new_value - v))\n",
    "                    self.state_values[i, j] = new_value\n",
    "\n",
    "    def calculate_value(self, i, j):\n",
    "        reward = -1\n",
    "        value = 0\n",
    "        transitions = self.get_transitions(i, j)\n",
    "        for (new_i, new_j) in transitions:\n",
    "            value += 0.25 * (reward + self.gamma * self.state_values[new_i, new_j])\n",
    "        return value\n",
    "\n",
    "    def get_transitions(self, i, j):\n",
    "        return [\n",
    "            (max(0, i - 1), j),\n",
    "            (min(self.size - 1, i + 1), j),\n",
    "            (i, max(0, j - 1)),\n",
    "            (i, min(self.size - 1, j + 1))\n",
    "        ]\n",
    "\n",
    "    def policy_improvement(self):\n",
    "        policy_stable = True\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                old_action = self.policy[i, j]\n",
    "                new_action = self.best_action(i, j)\n",
    "                self.policy[i, j] = new_action\n",
    "                if old_action != new_action:\n",
    "                    policy_stable = False\n",
    "        return policy_stable\n",
    "\n",
    "    def best_action(self, i, j):\n",
    "        actions = ['up', 'down', 'left', 'right']\n",
    "        values = [self.calculate_action_value(i, j, a) for a in actions]\n",
    "        return actions[np.argmax(values)]\n",
    "\n",
    "    def calculate_action_value(self, i, j, action):\n",
    "        reward = -1\n",
    "        new_i, new_j = self.get_new_position(i, j, action)\n",
    "        return reward + self.gamma * self.state_values[new_i, new_j]\n",
    "\n",
    "    def get_new_position(self, i, j, action):\n",
    "        if action == 'up':\n",
    "            return max(0, i - 1), j\n",
    "        elif action == 'down':\n",
    "            return min(self.size - 1, i + 1), j\n",
    "        elif action == 'left':\n",
    "            return i, max(0, j - 1)\n",
    "        else:\n",
    "            return i, min(self.size - 1, j + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925d00e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize gridworld and evaluate policy\n",
    "grid = Gridworld()\n",
    "grid.policy_evaluation()\n",
    "\n",
    "# Visualize state values\n",
    "plt.imshow(grid.state_values, cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title(\"State Values after Policy Evaluation\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
